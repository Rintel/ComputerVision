{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 7: Semantic Segmentation\n",
    "Heute werden wir einfache Netzwerkarchitekturen für \"Semantic Segmentation\" testen. Ziel ist es dieses Paper in den Grundzügen zu implementieren: https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf. Bitte lesen!\n",
    "\n",
    "## Daten\n",
    "\n",
    "Es gibt einige gute Datensätze, die ihr (bei gegebener Hardware) herunterladen und benutzen könnt. Für diejenigen, die auf CPUs rechnen gilt immer der Tip: Bilder downsamplen!\n",
    "\n",
    "Sucht Euch einen Satensatz aus: \n",
    "\n",
    "KITTI: http://www.cvlibs.net/download.php?file=data_semantics.zip (~300 MB, 200 Bilder)\n",
    "\n",
    "DUS: http://www.6d-vision.com/scene-labeling (~3 GB, 500 Bilder)\n",
    "\n",
    "MIT. http://sceneparsing.csail.mit.edu/ (~1 GB, links siehe auf Seite)\n",
    "\n",
    "## Exc. 7.1 Fully convolutional network, no downsampling\n",
    "Implementiere die in der Vorlesung besprochene Netzwerkarchitektur von aufeinanderfolgenden CONV-Schichten (stride=1, mit zero-padding), um eine Ausgabeschicht zu bekommen, die die Eingabegröße aufweist. Tip: die letzte CONV-Schicht sollte eine Tiefe haben, die zur Zahl der Klassen korrespondiert. Benutze den L2-Loss zum Labelbild (Achtung: ihr müsst dafür entweder das Labelbild oder den Ausgabetensor umformulieren).\n",
    "\n",
    "Trainiere das Netzwerk auf den von Dir gewählten Datensatz und zeige den Verlauf des Losses, und einige zufällig gewählte Beispielbilder mit ihren vorhergesagten Segmentierungen an. (**RESULT**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5144431f66e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbinarize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import binarize\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session, get_session\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import VGG16, decode_predictions\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import optimizers\n",
    "\n",
    "from skimage import color, io\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "def preprocess(img):\n",
    "    return color.hsv2rgb(img)\n",
    "\n",
    "def loadImageArray(path):\n",
    "    list = []\n",
    "    for f in os.listdir(path):\n",
    "        ext = os.path.splitext(f)[1]\n",
    "        if ext.lower() not in [\".png\"]:\n",
    "            continue\n",
    "        list.append(color.hsv2rgb(io.imread(os.path.join(path, f))))\n",
    "    return np.copy(list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_generator = ImageDataGenerator()\n",
    "label_generator = ImageDataGenerator()\n",
    "\n",
    "# Beispiel für den KITTI-Datensatz. Ich habe die 200 training samples in 180 train- und 20 testbilder\n",
    "# geteilt (macht 180 samples inkl. labels)\n",
    "# um uns das Leben leichter zu machen, Bilder heruntersamplen\n",
    "img_size = (38, 125)\n",
    "\n",
    "train_image_path = os.path.join(\"semantic_segmentation\", \"train\", \"images\")\n",
    "train_label_path = os.path.join(\"semantic_segmentation\", \"train\", \"labels\")\n",
    "\n",
    "print(train_image_path)\n",
    "\n",
    "train_images = loadImageArray(train_image_path + \"/images\")\n",
    "\n",
    "print(len(train_images))\n",
    "\n",
    "plt.figure(1, figsize=(20,7))\n",
    "plt.imshow(train_images[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (img_size[0], img_size[1], 3)\n",
    "lr = 0.001\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, img_size, activation=\"relu\", padding=\"same\", input_shape=input_shape))\n",
    "model.add(Conv2D(16, img_size, activation=\"relu\", padding=\"same\"))\n",
    "model.add(Conv2D(12, img_size, activation=\"relu\", padding=\"same\"))\n",
    "model.add(Conv2D(12, img_size, activation=\"relu\", padding=\"same\"))\n",
    "model.add(Conv2D(6, img_size, activation=\"relu\", padding=\"same\"))\n",
    "model.add(Conv2D(3, img_size, activation=\"relu\", padding=\"same\"))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=optimizers.Adam(lr=lr),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nach der Definition des Modells:\n",
    "# model.fit_generator(train_generator, steps_per_epoch = 1000, epochs = 10)\n",
    "model.fit_generator(train_generator, steps_per_epoch = 50, epochs = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-df2508513a5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/ss-4406.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/ss-weights-4406.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#model.load_weights('models/ss-weights-4406.h5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save('models/ss-4406.h5')\n",
    "model.save_weights('models/ss-weights-4406.h5')\n",
    "#model.load_weights('models/ss-weights-4406.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path = os.path.join(\"semantic_segmentation\", \"test\", \"images\")\n",
    "test_label_path = os.path.join(\"semantic_segmentation\", \"test\", \"labels\")\n",
    "\n",
    "testImg = image_generator.flow_from_directory(test_image_path,\n",
    "                                        class_mode=None,\n",
    "                                        target_size=img_size, \n",
    "                                        seed=1)\n",
    "\n",
    "testLbl = image_generator.flow_from_directory(test_label_path,\n",
    "                                        class_mode=None,\n",
    "                                        target_size=img_size, \n",
    "                                        seed=1)\n",
    "\n",
    "test = map(testImg, testLbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(testImg.next()[0].shape)\n",
    "image = testImg.next()[0,:,:,:]\n",
    "predictions = model.predict_generator(testImg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(20,7))\n",
    "img = preprocess(image.astype(int))\n",
    "plt.imshow(img[:,:,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions.shape)\n",
    "plt.figure(1, figsize=(20,7))\n",
    "plt.imshow(predictions[1,:,:,0].astype(int))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exc. 7.2 FCN mit Bottleneck\n",
    "\n",
    "Implementiere jetzt die Variante mit schrittweisem Down- und Upsampling, wie in der Vorlesung besprochen. Nutze dafür ein bestehendes Netzwerk (z.B. VGG16, https://keras.io/applications/#vgg16), entferne die FC-Schichten am Ende, und füge dann die Upsampling-Schichten hinzu. Wie in der vorigen Vorlesung zu Transfer Learning beschrieben, kannst Du jetzt nur den zweiten Teil trainieren und die Gewichte des ersten Teils \"einfrieren\".\n",
    "\n",
    "Stelle wie oben den Verlauf des Losses dar und wähle einige Beispielbilder aus dem Testset und zeige sie mit ihrer vorhergesagten Segmentierung an. (**BONUS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
